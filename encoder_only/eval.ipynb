{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3zslKf6tOIk"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets\n",
        "!pip install pytorch-lightning\n",
        "!pip install -q git+https://github.com/huggingface/peft.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XbxRa9efrQt3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from datasets import load_dataset\n",
        "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score, classification_report\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers import EvalPrediction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBLAqd7btXBL"
      },
      "outputs": [],
      "source": [
        "\n",
        "dataset = load_dataset('csv', data_files={'train': ['/content/drive/MyDrive/projects/finetuning_LLMs_with_MIRTE_data/data/cleaned_data_for_multiclassification_task/cleaned_shuffled_new_MITRE.csv',],\n",
        "                                          })\n",
        "labels = [label for label in dataset['train'].features.keys() if label not in ['ID', 'Description']]\n",
        "labels = sorted(labels)\n",
        "id2label = {idx:label for idx, label in enumerate(labels)}\n",
        "label2id = {label:idx for idx, label in enumerate(labels)}\n",
        "\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"roberta-large\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"ehsanaghaei/SecureBERT\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pzX7PKa14E_4"
      },
      "outputs": [],
      "source": [
        "test_set = pd.read_csv('/content/drive/MyDrive/projects/finetuning_LLMs_with_MIRTE_data/data/testing_data/MITRE_Procedures.csv')\n",
        "test_set = test_set.drop(columns=['URL'])\n",
        "unique_tactic_names = pd.Series(test_set['Tactic1'].tolist() + test_set['Tactic2'].tolist() + test_set['Tactic3'].tolist() + test_set['Tactic4'].tolist()).unique()\n",
        "my_list = [x for x in unique_tactic_names if (isinstance(x, str) or not math.isnan(x))]\n",
        "Tactic_column = ['Tactic1', 'Tactic2', 'Tactic3', 'Tactic4']\n",
        "for name in my_list:\n",
        "  test_set[name] = 0\n",
        "\n",
        "for i in range(len(test_set)):\n",
        "  for tactic in Tactic_column:\n",
        "    if pd.notna(test_set[tactic].iloc[i]):\n",
        "      test_set[test_set[tactic].iloc[i]].iloc[i] = 1\n",
        "\n",
        "test_set['Tactics'] = test_set.apply(lambda x: ', '.join(sorted([value for value in x[['Tactic1', 'Tactic2', 'Tactic3', 'Tactic4']] if pd.notnull(value)])), axis=1)\n",
        "test_set.drop(['Tactic1', 'Tactic2', 'Tactic3', 'Tactic4'], axis=1, inplace=True)\n",
        "\n",
        "desired_order = ['Procedures', 'COLLECTION', 'COMMAND_AND_CONTROL', 'CREDENTIAL_ACCESS',\n",
        " 'DEFENSE_EVASION', 'DISCOVERY', 'EXECUTION', 'EXFILTRATION', 'IMPACT', 'INITIAL_ACCESS',\n",
        " 'LATERAL_MOVEMENT', 'PERSISTENCE', 'PRIVILEGE_ESCALATION', 'RECONNAISSANCE', 'RESOURCE_DEVELOPMENT', 'Tactics']\n",
        "\n",
        "\n",
        "\n",
        "# Reindex the DataFrame with the desired column order\n",
        "test_set = test_set.reindex(columns=desired_order)\n",
        "\n",
        "for testing_number in range(1, 11):\n",
        "    data = []\n",
        "    df_predicted = pd.DataFrame(columns = test_set.columns[1:-1])\n",
        "    for i in range(len(test_set[\"Procedures\"])):\n",
        "\n",
        "        text = test_set['Procedures'][i]\n",
        "\n",
        "        encoding = tokenizer(text, return_tensors=\"pt\")\n",
        "        encoding = {k: v.to(trainer.model.device) for k,v in encoding.items()}\n",
        "\n",
        "        outputs = trainer.model(**encoding)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        sigmoid = torch.nn.Sigmoid()\n",
        "        probs = sigmoid(logits.squeeze().cpu())\n",
        "\n",
        "        predictions = np.zeros(probs.shape)\n",
        "        predictions[np.where(probs >= 0.5)] = 1\n",
        "        df_predicted.loc[len(df_predicted)] = predictions\n",
        "        predicted_labels = [id2label[idx] for idx, label in enumerate(predictions) if label == 1.0]\n",
        "\n",
        "        keys_with_value_1 = test_set['Tactics'][i]\n",
        "        data.append({'Description': text, 'Predicted_Labels': ', '.join(predicted_labels), 'Actual_labels': keys_with_value_1})\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    new_df = df.copy()\n",
        "    new_df['Predicted_Labels'] = new_df['Predicted_Labels'].apply(lambda x: ', '.join(sorted(x.split(', '))))\n",
        "    new_df['Predicted_Labels'] = new_df['Predicted_Labels'].str.upper()\n",
        "\n",
        "    for i in range(len(new_df['Predicted_Labels'])):\n",
        "        if len(new_df['Predicted_Labels'][i].split(', ')) != 1:\n",
        "            labels = new_df['Predicted_Labels'][i].split(', ')\n",
        "            modified_labels = [label.replace(' ', '_') for label in labels]\n",
        "            new_df['Predicted_Labels'][i] = ', '.join(modified_labels)\n",
        "        else:\n",
        "            new_df['Predicted_Labels'][i] = new_df['Predicted_Labels'][i].replace(' ', '_')\n",
        "\n",
        "    new_df['Match'] = new_df['Predicted_Labels'] == new_df['Actual_labels']\n",
        "    mismatched_rows = new_df[~new_df['Match']]\n",
        "    mismatch_count = len(mismatched_rows)\n",
        "    if mismatch_count > 0:\n",
        "        print(\"Mismatched Rows:\")\n",
        "        print(mismatched_rows)\n",
        "        print(f\"Total Mismatched Rows: {mismatch_count}\")\n",
        "    else:\n",
        "        print(\"No mismatches found.\")\n",
        "\n",
        "    # Save the DataFrame with a unique file name based on the testing number\n",
        "    predicted_labels = new_df['Predicted_Labels'].tolist()\n",
        "    actual_labels = new_df['Actual_labels'].tolist()\n",
        "    report = classification_report(actual_labels, predicted_labels, output_dict=True)\n",
        "    report_df = pd.DataFrame(report).transpose()\n",
        "\n",
        "    class_names = df_predicted.columns.tolist()\n",
        "    test_set_report = test_set.drop(columns=['Procedures', 'Tactics'])\n",
        "    y_true = test_set_report.values\n",
        "    y_pred = df_predicted.values\n",
        "    report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)\n",
        "    report = pd.DataFrame(report).transpose()\n",
        "\n",
        "    # Define the file paths with the testing number in the file names\n",
        "    result_file_path = f\"/content/drive/MyDrive/projects/finetuning_LLMs_with_MIRTE_data/result_of_testing/10times/roberta_base/predicted_labels_{testing_number}.csv\"\n",
        "    report_file_path = f\"/content/drive/MyDrive/projects/finetuning_LLMs_with_MIRTE_data/result_of_testing/10times/roberta_base/c_report_{testing_number}.csv\"\n",
        "    report_path = f\"/content/drive/MyDrive/projects/finetuning_LLMs_with_MIRTE_data/result_of_testing/10times/roberta_base/classification_report_{testing_number}.csv\"\n",
        "\n",
        "    new_df.to_csv(result_file_path, index=False)\n",
        "    report_df.to_csv(report_file_path, index=True)\n",
        "    report.to_csv(report_path, index=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
