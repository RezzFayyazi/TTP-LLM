# -*- coding: utf-8 -*-
"""finetuning_roberta_with_MITRE_dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ycarDJ6NHEKCWnifZEHGeMshX6ngsCyb
"""
"""
!pip install -q transformers datasets
!pip install pytorch-lightning
!pip install -q git+https://github.com/huggingface/peft.git
"""
import pandas as pd
import numpy as np
import random
import math
import torch
import torch.nn as nn
from datasets import load_dataset
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score, roc_auc_score, accuracy_score

from transformers import AutoTokenizer
from transformers import AutoModelForSequenceClassification
from transformers import TrainingArguments, Trainer
from transformers import EvalPrediction

state = 42
torch.manual_seed(state)
torch.cuda.manual_seed(state)
np.random.seed(state)
random.seed(state)
torch.backends.cudnn.enabled=False
torch.backends.cudnn.deterministic=True


from transformers import set_seed
set_seed(42)

from google.colab import drive
drive.mount('/content/drive')

#if we want use all data for training
dataset = load_dataset('csv', data_files={'train': ['/content/drive/MyDrive/projects/finetuning_LLMs_with_MIRTE_data/data/cleaned_data_for_multiclassification_task/cleaned_shuffled_new_MITRE.csv',],
                                          })
#if we want use splited data for training
# dataset = load_dataset('csv', data_files={'train': ['/content/drive/MyDrive/projects/finetuning_LLMs_with_MIRTE_data/data/cleaned_data_for_multiclassification_task/splited_data_into_train_test_val/cleaned_MITRE_data_trainset.csv',],
#                                           'test': ['/content/drive/MyDrive/projects/finetuning_LLMs_with_MIRTE_data/data/cleaned_data_for_multiclassification_task/splited_data_into_train_test_val/cleaned_MITRE_data_testset.csv',],
#                                           'validation': ['/content/drive/MyDrive/projects/finetuning_LLMs_with_MIRTE_data/data/cleaned_data_for_multiclassification_task/splited_data_into_train_test_val/cleaned_MITRE_data_valset.csv',]
#                                           })
dataset

"""Let's check the first example of the training split:"""

example = dataset['train'][0]
example

"""Let's create a list that contains the labels, as well as 2 dictionaries that map labels to integers and back."""

labels = [label for label in dataset['train'].features.keys() if label not in ['ID', 'Description']]
labels = sorted(labels)
id2label = {idx:label for idx, label in enumerate(labels)}
label2id = {label:idx for idx, label in enumerate(labels)}
labels

"""## Preprocess data

As models like BERT don't expect text as direct input, but rather `input_ids`, etc., we tokenize the text using the tokenizer. Here I'm using the `AutoTokenizer` API, which will automatically load the appropriate tokenizer based on the checkpoint on the hub.

What's a bit tricky is that we also need to provide labels to the model. For multi-label text classification, this is a matrix of shape (batch_size, num_labels). Also important: this should be a tensor of floats rather than integers, otherwise PyTorch' `BCEWithLogitsLoss` (which the model will use) will complain, as explained [here](https://discuss.pytorch.org/t/multi-label-binary-classification-result-type-float-cant-be-cast-to-the-desired-output-type-long/117915/3).
"""

#tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
#tokenizer = AutoTokenizer.from_pretrained("roberta-base")
tokenizer = AutoTokenizer.from_pretrained("ehsanaghaei/SecureBERT")

def preprocess_data(examples):
  # take a batch of texts
  text = examples["Description"]
  # encode them
  encoding = tokenizer(text, padding="max_length", truncation=True, max_length=512)
  # add labels
  labels_batch = {k: examples[k] for k in examples.keys() if k in labels}
  # create numpy array of shape (batch_size, num_labels)
  labels_matrix = np.zeros((len(text), len(labels)))
  # fill numpy array
  for idx, label in enumerate(labels):
    labels_matrix[:, idx] = labels_batch[label]

  encoding["labels"] = labels_matrix.tolist()

  return encoding

encoded_dataset = dataset.map(preprocess_data, batched=True, remove_columns=dataset['train'].column_names)

example = encoded_dataset['train'][0]
print(example.keys())

tokenizer.decode(example['input_ids'])

example['labels']

[id2label[idx] for idx, label in enumerate(example['labels']) if label == 1.0]

"""Finally, we set the format of our data to PyTorch tensors. This will turn the training, validation and test sets into standard PyTorch [datasets](https://pytorch.org/docs/stable/data.html)."""

encoded_dataset.set_format("torch")

"""## Define model

Here we define a model that includes a pre-trained base (i.e. the weights from bert-base-uncased) are loaded, with a random initialized classification head (linear layer) on top. One should fine-tune this head, together with the pre-trained base on a labeled dataset.

This is also printed by the warning.

We set the `problem_type` to be "multi_label_classification", as this will make sure the appropriate loss function is used (namely [`BCEWithLogitsLoss`](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html)). We also make sure the output layer has `len(labels)` output neurons, and we set the id2label and label2id mappings.
"""

model = AutoModelForSequenceClassification.from_pretrained("ehsanaghaei/SecureBERT",
                                                           problem_type="multi_label_classification",
                                                           num_labels=len(labels),
                                                           id2label=id2label,
                                                           label2id=label2id)

"""## Train the model!

We are going to train the model using HuggingFace's Trainer API. This requires us to define 2 things:

* `TrainingArguments`, which specify training hyperparameters. All options can be found in the [docs](https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments). Below, we for example specify that we want to evaluate after every epoch of training, we would like to save the model every epoch, we set the learning rate, the batch size to use for training/evaluation, how many epochs to train for, and so on.
* a `Trainer` object (docs can be found [here](https://huggingface.co/transformers/main_classes/trainer.html#id1)).
"""

batch_size = 16
metric_name = "f1"

#if you want to train the model without evaluation run this code
args = TrainingArguments(
    f"roberta-finetuned",
    evaluation_strategy = "no",
    do_eval=False,
    learning_rate=5e-5,
    per_device_train_batch_size=batch_size,
    num_train_epochs=30,
    weight_decay=0.01,
)


#if you want to train the model with evaluation run this code
# args = TrainingArguments(
#     f"roberta-finetuned",
#     evaluation_strategy = "epoch",
#     save_strategy = "epoch",
#     learning_rate=2e-5,
#     per_device_train_batch_size=batch_size,
#     per_device_eval_batch_size=batch_size,
#     num_train_epochs=12,
#     weight_decay=0.01,
#     load_best_model_at_end=True,
#     metric_for_best_model=metric_name,
# )

"""We are also going to compute metrics while training. For this, we need to define a `compute_metrics` function, that returns a dictionary with the desired metric values."""

# source: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/

def multi_label_metrics(predictions, labels, threshold=0.5):
    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)
    sigmoid = torch.nn.Sigmoid()
    probs = sigmoid(torch.Tensor(predictions))
    # next, use threshold to turn them into integer predictions
    y_pred = np.zeros(probs.shape)
    y_pred[np.where(probs >= threshold)] = 1
    # finally, compute metrics
    y_true = labels
    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')
    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')
    accuracy = accuracy_score(y_true, y_pred)
    # return as dictionary
    metrics = {'f1': f1_micro_average,
               'roc_auc': roc_auc,
               'accuracy': accuracy}
    return metrics

def compute_metrics(p: EvalPrediction):
    preds = p.predictions[0] if isinstance(p.predictions,
            tuple) else p.predictions
    result = multi_label_metrics(
        predictions=preds,
        labels=p.label_ids)
    return result

"""Let's verify a batch as well as a forward pass:

Let's start training!
"""

#if we want to evaluate our traing with val or test set we can uncomment eval_dataset, and compute_metrics
trainer = Trainer(
    model,
    args,
    train_dataset=encoded_dataset["train"],
    #eval_dataset=encoded_dataset["test"],
    tokenizer=tokenizer,
    #compute_metrics=compute_metrics,
    )

trainer.train()

trainer.save_model('/content/drive/MyDrive/projects/finetuning_LLMs_with_MIRTE_data/trained_models/new_MITRE_dataset/secure_bert_30epochs_16b_5e-5L')

#if we use test or eval set we can use this line of code for evaluation
#trainer.evaluate()

#previous version
from sklearn.metrics import classification_report


test_set = pd.read_csv('/content/drive/MyDrive/projects/finetuning_LLMs_with_MIRTE_data/data/testing_data/Y-MITRE_Procedures.csv')
test_set['Tactics'] = test_set.apply(lambda x: ', '.join(sorted([value for value in x[['Tactic1', 'Tactic2', 'Tactic3', 'Tactic4']] if pd.notnull(value)])), axis=1)
test_set.drop(['Tactic1', 'Tactic2', 'Tactic3', 'Tactic4'], axis=1, inplace=True)

data = []

for i in range(len(test_set["Procedures"])):
    text = test_set['Procedures'][i]

    encoding = tokenizer(text, return_tensors="pt")
    encoding = {k: v.to(trainer.model.device) for k,v in encoding.items()}

    outputs = trainer.model(**encoding)
    logits = outputs.logits

    sigmoid = torch.nn.Sigmoid()
    probs = sigmoid(logits.squeeze().cpu())

    predictions = np.zeros(probs.shape)
    predictions[np.where(probs >= 0.5)] = 1
    predicted_labels = [id2label[idx] for idx, label in enumerate(predictions) if label == 1.0]

    keys_with_value_1 = test_set['Tactics'][i]
    data.append({'Description': text, 'Predicted_Labels': ', '.join(predicted_labels), 'Actual_labels': keys_with_value_1})

df = pd.DataFrame(data)

new_df = df.copy()
new_df['Predicted_Labels'] = new_df['Predicted_Labels'].apply(lambda x: ', '.join(sorted(x.split(', '))))
new_df['Predicted_Labels'] = new_df['Predicted_Labels'].str.upper()

for i in range(len(new_df['Predicted_Labels'])):
    if len(new_df['Predicted_Labels'][i].split(', ')) != 1:
        labels = new_df['Predicted_Labels'][i].split(', ')
        modified_labels = [label.replace(' ', '_') for label in labels]
        new_df['Predicted_Labels'][i] = ', '.join(modified_labels)
    else:
      new_df['Predicted_Labels'][i] = new_df['Predicted_Labels'][i].replace(' ', '_')


new_df['Match'] = new_df['Predicted_Labels'] == new_df['Actual_labels']
mismatched_rows = new_df[~new_df['Match']]
mismatch_count = len(mismatched_rows)
if mismatch_count > 0:
    print("Mismatched Rows:")
    print(mismatched_rows)
    print(f"Total Mismatched Rows: {mismatch_count}")
else:
    print("No mismatches found.")
new_df.to_csv('/content/drive/MyDrive/projects/finetuning_LLMs_with_MIRTE_data/result_of_testing/testing_seed_number/roberta_base/16batches_30epochs/3rd_secure_predicted_labels_0.5_5e-5.csv', index=False)


predicted_labels = new_df['Predicted_Labels'].tolist()
actual_labels = new_df['Actual_labels'].tolist()
report = classification_report(actual_labels, predicted_labels, output_dict=True)
report_df = pd.DataFrame(report).transpose()

report_df.to_csv('/content/drive/MyDrive/projects/finetuning_LLMs_with_MIRTE_data/result_of_testing/testing_seed_number/roberta_base/16batches_30epochs/3rd_secure_classification_report_0.5_5e-5.csv', index=True)

from sklearn.metrics import classification_report

test_set = pd.read_csv('/content/drive/MyDrive/projects/finetuning_LLMs_with_MIRTE_data/data/testing_data/Y-MITRE_Procedures.csv')
test_set = test_set.drop(columns=['URL'])
unique_tactic_names = pd.Series(test_set['Tactic1'].tolist() + test_set['Tactic2'].tolist() + test_set['Tactic3'].tolist() + test_set['Tactic4'].tolist()).unique()
my_list = [x for x in unique_tactic_names if (isinstance(x, str) or not math.isnan(x))]
Tactic_column = ['Tactic1', 'Tactic2', 'Tactic3', 'Tactic4']
for name in my_list:
  test_set[name] = 0

for i in range(len(test_set)):
  for tactic in Tactic_column:
    if pd.notna(test_set[tactic].iloc[i]):
      test_set[test_set[tactic].iloc[i]].iloc[i] = 1

test_set['Tactics'] = test_set.apply(lambda x: ', '.join(sorted([value for value in x[['Tactic1', 'Tactic2', 'Tactic3', 'Tactic4']] if pd.notnull(value)])), axis=1)
test_set.drop(['Tactic1', 'Tactic2', 'Tactic3', 'Tactic4'], axis=1, inplace=True)

desired_order = ['Procedures', 'COLLECTION', 'COMMAND_AND_CONTROL', 'CREDENTIAL_ACCESS',
 'DEFENSE_EVASION', 'DISCOVERY', 'EXECUTION', 'EXFILTRATION', 'IMPACT', 'INITIAL_ACCESS',
 'LATERAL_MOVEMENT', 'PERSISTENCE', 'PRIVILEGE_ESCALATION', 'RECONNAISSANCE', 'RESOURCE_DEVELOPMENT', 'Tactics']



# Reindex the DataFrame with the desired column order
test_set = test_set.reindex(columns=desired_order)

for testing_number in range(1, 11):
    data = []
    df_predicted = pd.DataFrame(columns = test_set.columns[1:-1])
    for i in range(len(test_set["Procedures"])):

        text = test_set['Procedures'][i]

        encoding = tokenizer(text, return_tensors="pt")
        encoding = {k: v.to(trainer.model.device) for k,v in encoding.items()}

        outputs = trainer.model(**encoding)
        logits = outputs.logits

        sigmoid = torch.nn.Sigmoid()
        probs = sigmoid(logits.squeeze().cpu())

        predictions = np.zeros(probs.shape)
        predictions[np.where(probs >= 0.5)] = 1
        df_predicted.loc[len(df_predicted)] = predictions
        predicted_labels = [id2label[idx] for idx, label in enumerate(predictions) if label == 1.0]

        keys_with_value_1 = test_set['Tactics'][i]
        data.append({'Description': text, 'Predicted_Labels': ', '.join(predicted_labels), 'Actual_labels': keys_with_value_1})

    df = pd.DataFrame(data)

    new_df = df.copy()
    new_df['Predicted_Labels'] = new_df['Predicted_Labels'].apply(lambda x: ', '.join(sorted(x.split(', '))))
    new_df['Predicted_Labels'] = new_df['Predicted_Labels'].str.upper()

    for i in range(len(new_df['Predicted_Labels'])):
        if len(new_df['Predicted_Labels'][i].split(', ')) != 1:
            labels = new_df['Predicted_Labels'][i].split(', ')
            modified_labels = [label.replace(' ', '_') for label in labels]
            new_df['Predicted_Labels'][i] = ', '.join(modified_labels)
        else:
            new_df['Predicted_Labels'][i] = new_df['Predicted_Labels'][i].replace(' ', '_')

    new_df['Match'] = new_df['Predicted_Labels'] == new_df['Actual_labels']
    mismatched_rows = new_df[~new_df['Match']]
    mismatch_count = len(mismatched_rows)
    if mismatch_count > 0:
        print("Mismatched Rows:")
        print(mismatched_rows)
        print(f"Total Mismatched Rows: {mismatch_count}")
    else:
        print("No mismatches found.")

    # Save the DataFrame with a unique file name based on the testing number
    predicted_labels = new_df['Predicted_Labels'].tolist()
    actual_labels = new_df['Actual_labels'].tolist()
    report = classification_report(actual_labels, predicted_labels, output_dict=True)
    report_df = pd.DataFrame(report).transpose()

    class_names = df_predicted.columns.tolist()
    test_set_report = test_set.drop(columns=['Procedures', 'Tactics'])
    y_true = test_set_report.values
    y_pred = df_predicted.values
    report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)
    report = pd.DataFrame(report).transpose()

    # Define the file paths with the testing number in the file names
    result_file_path = f"/content/drive/MyDrive/projects/finetuning_LLMs_with_MIRTE_data/result_of_testing/10times/roberta_base/predicted_labels_{testing_number}.csv"
    report_file_path = f"/content/drive/MyDrive/projects/finetuning_LLMs_with_MIRTE_data/result_of_testing/10times/roberta_base/c_report_{testing_number}.csv"
    report_path = f"/content/drive/MyDrive/projects/finetuning_LLMs_with_MIRTE_data/result_of_testing/10times/roberta_base/classification_report_{testing_number}.csv"

    new_df.to_csv(result_file_path, index=False)
    report_df.to_csv(report_file_path, index=True)
    report.to_csv(report_path, index=True)

